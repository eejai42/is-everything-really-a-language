#!/usr/bin/env python3
"""
ExplainDAG Test Runner - Evaluate formulas and produce derivation explanations.

This script:
1. Loads explain_spec.json (generated by inject-into-explain-dag.py)
2. Processes blank-tests for each entity
3. Evaluates formulas in DAG order
4. Produces:
   - test-answers/<Entity>.json (for grading, same as other substrates)
   - test-explanations/<Entity>.jsonl (derivation DAGs with witnessed values)

The key feature: every calculated value comes with a machine-readable
explanation of exactly how it was derived.
"""

import json
import os
import sys
import glob
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Add project root to path
script_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(script_dir.parent.parent))

from orchestration.shared import to_snake_case


# =============================================================================
# EVALUATION ENGINE WITH EXPLANATION
# =============================================================================

@dataclass
class EvalResult:
    """Result of evaluating a node, including the witnessed value."""
    value: Any
    type: str
    explanation: Dict[str, Any]


class ExplainEvaluator:
    """
    Evaluates AST templates against record values, producing explanations.

    Uses Excel-style semantics with three-valued logic for null handling.
    """

    def __init__(self, semantics: Dict[str, Any]):
        self.semantics = semantics
        self.profile = semantics.get("profile", "excel")

    def evaluate_template(
        self,
        template: Dict[str, Any],
        record: Dict[str, Any],
        field_name: str
    ) -> tuple:
        """
        Evaluate a template against a record.

        Returns:
        - computed_value: the final value
        - explanation: the witnessed derivation DAG
        """
        nodes = template.get("nodes", {})
        root_node = template.get("root_node")

        if not root_node or not nodes:
            return None, {"error": "Invalid template"}

        # Track witnessed values for each node
        witnessed = {}
        instance_nodes = {}
        instance_edges = []

        # Get evaluation order (reverse topological from edges)
        eval_order = self._compute_eval_order(nodes, template.get("edges", []))

        # Evaluate each node in order
        for node_id in eval_order:
            node_def = nodes.get(node_id)
            if not node_def:
                continue

            result = self._eval_node(node_id, node_def, witnessed, record)
            witnessed[node_id] = result

            # Build instance node
            instance_id = f"i_{node_id[2:]}" if node_id.startswith("n_") else f"i_{node_id}"
            instance_nodes[instance_id] = {
                "kind": result.explanation.get("kind", node_def.get("kind")),
                "value": result.value,
                "type": result.type,
                **{k: v for k, v in result.explanation.items() if k not in ("kind", "value", "type")}
            }

        # Build instance edges (same structure as template)
        for edge in template.get("edges", []):
            src_id = f"i_{edge[0][2:]}" if edge[0].startswith("n_") else f"i_{edge[0]}"
            tgt_id = f"i_{edge[1][2:]}" if edge[1].startswith("n_") else f"i_{edge[1]}"
            instance_edges.append([src_id, tgt_id])

        # Get the final value from root
        root_result = witnessed.get(root_node)
        computed_value = root_result.value if root_result else None

        # Build explanation structure
        instance_root = f"i_{root_node[2:]}" if root_node.startswith("n_") else f"i_{root_node}"
        explanation = {
            "template_hash": template.get("template_hash"),
            "root": instance_root,
            "nodes": instance_nodes,
            "edges": instance_edges,
            "evidence": {
                "short_circuit": [],
                "branches": []
            }
        }

        return computed_value, explanation

    def _compute_eval_order(self, nodes: Dict, edges: List) -> List[str]:
        """Compute evaluation order (dependencies before dependents)."""
        # Build adjacency for reverse topological sort
        deps = {node_id: set() for node_id in nodes}
        for src, tgt in edges:
            if tgt in deps:
                deps[tgt].add(src)

        # Kahn's algorithm
        order = []
        ready = [n for n, d in deps.items() if not d]
        while ready:
            node = ready.pop(0)
            order.append(node)
            for other, other_deps in deps.items():
                if node in other_deps:
                    other_deps.remove(node)
                    if not other_deps and other not in order:
                        ready.append(other)

        # Add any remaining (handles cycles gracefully)
        for node_id in nodes:
            if node_id not in order:
                order.append(node_id)

        return order

    def _eval_node(
        self,
        node_id: str,
        node_def: Dict[str, Any],
        witnessed: Dict[str, EvalResult],
        record: Dict[str, Any]
    ) -> EvalResult:
        """Evaluate a single node."""
        kind = node_def.get("kind")

        if kind == "const":
            return EvalResult(
                value=node_def.get("value"),
                type=node_def.get("type", "unknown"),
                explanation={"kind": "const", "value": node_def.get("value")}
            )

        if kind == "field_ref":
            field_snake = node_def.get("field_snake", to_snake_case(node_def.get("field", "")))
            value = record.get(field_snake)
            return EvalResult(
                value=value,
                type=self._infer_type(value),
                explanation={"kind": "field_ref", "field": node_def.get("field"), "field_snake": field_snake}
            )

        if kind == "result":
            # Result node just wraps its input
            input_ids = node_def.get("in", [])
            if input_ids and input_ids[0] in witnessed:
                input_result = witnessed[input_ids[0]]
                return EvalResult(
                    value=input_result.value,
                    type=input_result.type,
                    explanation={"kind": "result", "field": node_def.get("field")}
                )
            return EvalResult(value=None, type="null", explanation={"kind": "result", "error": "missing input"})

        if kind == "fn":
            return self._eval_function(node_def, witnessed)

        if kind == "op":
            return self._eval_operator(node_def, witnessed)

        return EvalResult(value=None, type="null", explanation={"kind": kind, "error": "unknown kind"})

    def _eval_function(self, node_def: Dict, witnessed: Dict[str, EvalResult]) -> EvalResult:
        """Evaluate a function call."""
        name = node_def.get("name", "").upper()
        arg_ids = node_def.get("args", [])
        args = [witnessed.get(aid) for aid in arg_ids]
        arg_values = [a.value if a else None for a in args]

        result_value = None
        result_type = "null"

        if name == "AND":
            # AND with null handling: False if any False, None if any None (and no False), True otherwise
            if any(v is False for v in arg_values):
                result_value = False
            elif any(v is None for v in arg_values):
                result_value = None
            else:
                result_value = all(v is True for v in arg_values)
            result_type = "boolean"

        elif name == "OR":
            # OR with null handling: True if any True, None if any None (and no True), False otherwise
            if any(v is True for v in arg_values):
                result_value = True
            elif any(v is None for v in arg_values):
                result_value = None
            else:
                result_value = any(v is True for v in arg_values)
            result_type = "boolean"

        elif name == "NOT":
            if len(arg_values) >= 1:
                v = arg_values[0]
                if v is None:
                    result_value = None
                else:
                    result_value = not bool(v)
            result_type = "boolean"

        elif name == "IF":
            if len(arg_values) >= 2:
                cond = arg_values[0]
                then_val = arg_values[1]
                else_val = arg_values[2] if len(arg_values) > 2 else None
                if cond is True:
                    result_value = then_val
                else:
                    result_value = else_val
                result_type = self._infer_type(result_value)

        elif name == "LOWER":
            if len(arg_values) >= 1 and arg_values[0] is not None:
                result_value = str(arg_values[0]).lower()
                result_type = "string"

        elif name == "FIND":
            if len(arg_values) >= 2:
                needle = arg_values[0]
                haystack = arg_values[1]
                if needle is not None and haystack is not None:
                    result_value = str(needle) in str(haystack)
                result_type = "boolean"

        elif name == "CAST":
            if len(arg_values) >= 1:
                v = arg_values[0]
                result_value = str(v) if v is not None else ""
                result_type = "string"

        return EvalResult(
            value=result_value,
            type=result_type,
            explanation={"kind": "fn", "name": name, "args": arg_ids}
        )

    def _eval_operator(self, node_def: Dict, witnessed: Dict[str, EvalResult]) -> EvalResult:
        """Evaluate an operator."""
        name = node_def.get("name", "")
        arg_ids = node_def.get("args", [])
        args = [witnessed.get(aid) for aid in arg_ids]
        arg_values = [a.value if a else None for a in args]

        result_value = None
        result_type = "null"

        if name == "CONCAT":
            # Concatenate all parts, treating None as empty string
            parts = [str(v) if v is not None else "" for v in arg_values]
            result_value = "".join(parts)
            result_type = "string"

        elif name in ("=", "=="):
            if len(arg_values) >= 2:
                result_value = arg_values[0] == arg_values[1]
                result_type = "boolean"

        elif name in ("<>", "!="):
            if len(arg_values) >= 2:
                result_value = arg_values[0] != arg_values[1]
                result_type = "boolean"

        elif name == "<":
            if len(arg_values) >= 2 and arg_values[0] is not None and arg_values[1] is not None:
                result_value = arg_values[0] < arg_values[1]
                result_type = "boolean"

        elif name == "<=":
            if len(arg_values) >= 2 and arg_values[0] is not None and arg_values[1] is not None:
                result_value = arg_values[0] <= arg_values[1]
                result_type = "boolean"

        elif name == ">":
            if len(arg_values) >= 2 and arg_values[0] is not None and arg_values[1] is not None:
                result_value = arg_values[0] > arg_values[1]
                result_type = "boolean"

        elif name == ">=":
            if len(arg_values) >= 2 and arg_values[0] is not None and arg_values[1] is not None:
                result_value = arg_values[0] >= arg_values[1]
                result_type = "boolean"

        return EvalResult(
            value=result_value,
            type=result_type,
            explanation={"kind": "op", "name": name, "args": arg_ids}
        )

    def _infer_type(self, value: Any) -> str:
        """Infer the type of a value."""
        if value is None:
            return "null"
        if isinstance(value, bool):
            return "boolean"
        if isinstance(value, int):
            return "integer"
        if isinstance(value, float):
            return "number"
        if isinstance(value, str):
            return "string"
        return "unknown"


# =============================================================================
# SELF-VALIDATION
# =============================================================================

def validate_explanation(explanation: Dict, computed_value: Any) -> Optional[str]:
    """
    Validate that an explanation DAG is internally consistent.

    Returns None if valid, error message if invalid.
    """
    nodes = explanation.get("nodes", {})
    root = explanation.get("root")

    if not root or root not in nodes:
        return f"Root node {root} not found in nodes"

    root_node = nodes[root]
    root_value = root_node.get("value")

    if root_value != computed_value:
        return f"Root value {root_value} doesn't match computed {computed_value}"

    return None


# =============================================================================
# MAIN PROCESSING
# =============================================================================

def process_entity(
    entity_name: str,
    entity_spec: Dict[str, Any],
    input_path: Path,
    answers_path: Path,
    explanations_path: Path,
    semantics: Dict[str, Any]
) -> int:
    """Process a single entity, producing answers and explanations."""

    with open(input_path, 'r') as f:
        records = json.load(f)

    evaluator = ExplainEvaluator(semantics)
    templates = entity_spec.get("expr_templates", {})
    calc_order = entity_spec.get("calc_order", [])
    id_field = entity_spec.get("id_field_snake")
    rulebook_hash = entity_spec.get("rulebook_hash", "")

    computed_records = []
    explanation_bundles = []

    for record in records:
        # Start with a copy of the record
        result = dict(record)
        record_explanations = {}

        # Compute each calculated field in order
        for field_name in calc_order:
            template = templates.get(field_name)
            if not template or template.get("error"):
                continue

            # Evaluate the template
            computed_value, explanation = evaluator.evaluate_template(
                template, result, field_name
            )

            # Store the result
            field_snake = to_snake_case(field_name)
            result[field_snake] = computed_value

            # Store the explanation
            record_explanations[field_name] = explanation

            # Self-validate
            error = validate_explanation(explanation, computed_value)
            if error:
                explanation["validation_error"] = error

        computed_records.append(result)

        # Build explanation bundle
        record_id = result.get(id_field, "unknown")
        bundle = {
            "schema_version": "erb.explain_instance.v1",
            "rulebook_hash": rulebook_hash,
            "entity": entity_name,
            "record_id": record_id,
            "values": {to_snake_case(k): result.get(to_snake_case(k)) for k in calc_order},
            "explanations": record_explanations
        }
        explanation_bundles.append(bundle)

    # Write answers (for grading)
    with open(answers_path, 'w') as f:
        json.dump(computed_records, f, indent=2)

    # Write explanations (JSONL format)
    with open(explanations_path, 'w') as f:
        for bundle in explanation_bundles:
            f.write(json.dumps(bundle) + "\n")

    return len(records)


def main():
    project_root = script_dir.parent.parent
    blank_tests_dir = project_root / "testing" / "blank-tests"
    test_answers_dir = script_dir / "test-answers"
    test_explanations_dir = script_dir / "test-explanations"
    spec_path = script_dir / "generated" / "explain_spec.json"

    # Check for spec file
    if not spec_path.exists():
        print(f"Error: {spec_path} not found. Run inject-into-explain-dag.py first.")
        sys.exit(1)

    # Load spec
    with open(spec_path, 'r') as f:
        spec = json.load(f)

    semantics = spec.get("semantics", {})
    entities = spec.get("entities", {})
    rulebook_hash = spec.get("rulebook", {}).get("rulebook_hash", "")

    # Ensure output directories exist
    test_answers_dir.mkdir(exist_ok=True)
    test_explanations_dir.mkdir(exist_ok=True)

    # Process each entity
    total_records = 0
    entity_count = 0

    for input_path in sorted(blank_tests_dir.glob("*.json")):
        filename = input_path.name

        # Skip metadata files
        if filename.startswith('_'):
            continue

        entity_name = filename.replace('.json', '')

        # Check if we have a spec for this entity
        entity_spec = None
        for spec_entity in entities:
            if to_snake_case(spec_entity) == to_snake_case(entity_name):
                entity_spec = entities[spec_entity]
                entity_spec["rulebook_hash"] = rulebook_hash
                break

        if not entity_spec:
            # No calculated fields for this entity, just copy
            with open(input_path, 'r') as f:
                records = json.load(f)
            with open(test_answers_dir / filename, 'w') as f:
                json.dump(records, f, indent=2)
            print(f"  -> {entity_name}: {len(records)} records (no calculated fields)")
            continue

        answers_path = test_answers_dir / filename
        explanations_path = test_explanations_dir / f"{entity_name}.jsonl"

        count = process_entity(
            entity_name,
            entity_spec,
            input_path,
            answers_path,
            explanations_path,
            semantics
        )
        total_records += count
        entity_count += 1

        print(f"  -> {entity_name}: {count} records with explanations")

    print(f"ExplainDAG substrate: Processed {entity_count} entities, {total_records} total records")
    print(f"Explanations written to: {test_explanations_dir}")


if __name__ == "__main__":
    main()
