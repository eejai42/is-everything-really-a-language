<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Substrate Report: english</title>
    <style>
:root {
    --bg-primary: #ffffff; --bg-secondary: #f8f9fa; --bg-tertiary: #e9ecef;
    --text-primary: #212529; --text-secondary: #6c757d; --border-color: #dee2e6;
    --accent-color: #0d6efd; --success-color: #198754; --warning-color: #ffc107;
    --danger-color: #dc3545; --code-bg: #f6f8fa; --shadow: 0 2px 8px rgba(0,0,0,0.1); --radius: 6px;
}
[data-theme="dark"] {
    --bg-primary: #1a1a2e; --bg-secondary: #16213e; --bg-tertiary: #0f3460;
    --text-primary: #eaeaea; --text-secondary: #b0b0b0; --border-color: #3a3a5c;
    --accent-color: #4dabf7; --success-color: #51cf66; --warning-color: #fcc419;
    --danger-color: #ff6b6b; --code-bg: #0d1117; --shadow: 0 2px 8px rgba(0,0,0,0.3);
}
* { margin: 0; padding: 0; box-sizing: border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: var(--bg-secondary); color: var(--text-primary); line-height: 1.5; min-height: 100vh; }
header { background: var(--bg-primary); border-bottom: 1px solid var(--border-color); padding: 0.75rem 1rem; display: flex; justify-content: space-between; align-items: center; }
.header-left { display: flex; align-items: center; gap: 0.75rem; }
.substrate-icon { font-size: 1.5rem; }
h1 { font-size: 1.1rem; font-weight: 600; }
.header-stats { display: flex; gap: 1rem; font-size: 0.8rem; }
.stat { display: flex; align-items: center; gap: 0.25rem; }
.stat-value { font-weight: 600; }
.score-perfect, .score-good { color: var(--success-color); }
.score-warning { color: var(--warning-color); }
.score-danger { color: var(--danger-color); }
#theme-toggle { background: var(--bg-tertiary); border: 1px solid var(--border-color); border-radius: 50%; width: 28px; height: 28px; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 0.9rem; }
#theme-toggle:hover { background: var(--accent-color); color: white; }
#theme-toggle .moon { display: none; }
[data-theme="dark"] #theme-toggle .sun { display: none; }
[data-theme="dark"] #theme-toggle .moon { display: inline; }
.tabs { display: flex; background: var(--bg-primary); border-bottom: 1px solid var(--border-color); overflow-x: auto; padding: 0 0.5rem; }
.tab { background: none; border: none; padding: 0.5rem 1rem; cursor: pointer; font-size: 0.8rem; color: var(--text-secondary); border-bottom: 2px solid transparent; white-space: nowrap; }
.tab:hover { color: var(--text-primary); }
.tab.active { color: var(--accent-color); border-bottom-color: var(--accent-color); font-weight: 500; }
main { padding: 1rem; }
.tab-content { display: none; }
.tab-content.active { display: block; animation: fadeIn 0.2s ease; }
@keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
.card { background: var(--bg-primary); border: 1px solid var(--border-color); border-radius: var(--radius); padding: 1rem; margin-bottom: 1rem; box-shadow: var(--shadow); }
.card h2 { font-size: 0.9rem; font-weight: 600; margin-bottom: 0.75rem; }
pre { background: var(--code-bg); border: 1px solid var(--border-color); border-radius: var(--radius); padding: 0.75rem; overflow-x: auto; font-family: 'SF Mono', Monaco, monospace; font-size: 0.75rem; line-height: 1.4; max-height: 500px; overflow-y: auto; }
.code-info { font-size: 0.75rem; color: var(--text-secondary); margin-bottom: 0.5rem; }
.markdown-content { line-height: 1.7; }
.markdown-content h2 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem 0; border-bottom: 1px solid var(--border-color); padding-bottom: 0.5rem; }
.markdown-content h3 { font-size: 1rem; font-weight: 600; margin: 1.25rem 0 0.5rem 0; }
.markdown-content h4 { font-size: 0.9rem; font-weight: 600; margin: 1rem 0 0.5rem 0; color: var(--text-secondary); }
.markdown-content p { margin-bottom: 0.75rem; }
.markdown-content ul { margin-left: 1.5rem; margin-bottom: 0.75rem; }
.markdown-content li { margin-bottom: 0.35rem; }
.markdown-content pre { margin: 0.75rem 0; }
.markdown-content strong { font-weight: 600; }
.results-summary { display: flex; gap: 1.5rem; flex-wrap: wrap; margin-bottom: 1rem; }
.result-item { text-align: center; }
.result-value { font-size: 1.5rem; font-weight: 700; }
.result-label { font-size: 0.7rem; color: var(--text-secondary); text-transform: uppercase; }
.prose-container { max-height: 600px; overflow-y: auto; padding-right: 0.5rem; }
    </style>
</head>
<body>
    <header>
        <div class="header-left">
            <span class="substrate-icon">üìù</span>
            <h1>English (NLG) Execution Substrate</h1>
        </div>
        <div class="header-stats">
            <div class="stat"><span>Score:</span><span class="stat-value score-warning">71.7%</span></div>
            <div class="stat"><span>132/184 passed</span></div>
        </div>
        <button id="theme-toggle" title="Toggle theme"><span class="sun">‚òÄÔ∏è</span><span class="moon">üåô</span></button>
    </header>
    <nav class="tabs">
        <button class="tab active" data-tab="description">Description</button>
        <button class="tab" data-tab="log">Run Log</button>
        <button class="tab" data-tab="results">Test Results</button>
        <button class="tab" data-tab="specification">Specification</button>
    </nav>
    <main>
        <div id="description" class="tab-content active">
            <div class="card">
                <h2>What This Substrate Does</h2>
                <div class="markdown-content">
                    <p>The English substrate tests whether <strong>plain English can serve as an execution substrate</strong>. It uses a two-step LLM flow:</p>
                    <h3>Two-Step LLM Architecture</h3>
                    <ol>
                        <li><strong>Inject</strong>: <code>inject-into-english.py</code> sends the rulebook JSON to an LLM, which writes a plain English specification explaining how to compute each calculated field</li>
                        <li><strong>Take Test</strong>: <code>take-test.py</code> sends the English specification + test data to an LLM, which computes the calculated field values by following the English instructions</li>
                    </ol>
                    <p><strong>Zero formula parsing.</strong> The LLM reads formulas natively and explains them better than regex could.</p>
                </div>
            </div>
            <div class="card">
                <h2>Key Features</h2>
                <ul class="markdown-content">
                    <li><strong>Generic</strong>: Works with ANY rulebook - no domain-specific hardcoding</li>
                    <li><strong>LLM-Driven</strong>: Specification generated by LLM, not template strings</li>
                    <li><strong>Round-Trip Testing</strong>: Verifies the English spec is clear enough to compute correct values</li>
                    <li><strong>Minimal Code</strong>: ~130 lines total, down from ~425 lines</li>
                </ul>
            </div>
        </div>
        <div id="log" class="tab-content">
            <div class="card">
                <h2>Execution Log</h2>
                <pre>=== English (LLM) Substrate Test Run ===

======================================================================
English Execution Substrate - LLM Test Execution
Using ENGLISH PROSE documents (glossary.md, specification.md)
======================================================================

Loading English specification documents...
  Loaded specification.md (7173 chars)

Loading rulebook for schema metadata...
Discovered 3 entities: LanguageCandidates, IsEverythingALanguage, ERBCustomizations

Processing language_candidates...
  Schema: 26 fields, 8 calculated
    Using English prose documents (specification)
============================================================
PROMPT (first 2 lines):
  You are taking a test. Your task is to fill in the computed columns for each record based on the Eng
  
  ... (820 total lines, 28557 chars)
============================================================

  This will call OpenAI (gpt-4o-mini).
  Proceed with LLM call? [y/N]: Calling OpenAI (gpt-4o-mini)... please wait...
============================================================
RESPONSE (first 2 lines):
  [
    {
  ... (646 total lines, 25018 chars)
============================================================
  -&gt; language_candidates: 23 records, 184 computed fields filled

======================================================================
English substrate: Processed 1 entities, 23 total records
Filled 184 computed fields total
======================================================================

</pre>
            </div>
        </div>
        <div id="results" class="tab-content">
            <div class="card">
                <h2>Test Summary</h2>
                <div class="results-summary">
                    <div class="result-item"><div class="result-value score-warning">71.7%</div><div class="result-label">Score</div></div>
                    <div class="result-item"><div class="result-value score-perfect">132</div><div class="result-label">Passed</div></div>
                    <div class="result-item"><div class="result-value">52</div><div class="result-label">Failed</div></div>
                    <div class="result-item"><div class="result-value">184</div><div class="result-label">Total</div></div>
                </div>
            </div>
        </div>
        <div id="specification" class="tab-content">
            <div class="card">
                <h2>specification.md (LLM-Generated)</h2>
                <div class="prose-container markdown-content"><h2>Specification Document for Rulebook: PUBLISHED - ERB_semiotics-is-everything-a-language</h2>
<h3>Overview</h3>
<p>This rulebook provides a structured framework for evaluating various language candidates based on specific criteria. It includes a set of calculated fields that derive insights about whether a candidate qualifies as a language, based on attributes such as syntax, parsing, and identity. The rulebook is generated from an Airtable base and includes detailed descriptions of each field, along with the necessary formulas for calculating derived values.</p>
<h3>Entity: LanguageCandidates</h3>
<h4>Input Fields (Type: Raw)</h4>
<p>1. <strong>LanguageCandidateId</strong></p>
<p>   - <strong>Type:</strong> String</p>
<p>   - <strong>Description:</strong> Unique identifier for the language candidate.</p>
<p>2. <strong>Name</strong></p>
<p>   - <strong>Type:</strong> String</p>
<p>   - <strong>Description:</strong> Name of the language candidate being classified.</p>
<p>3. <strong>IsLanguage</strong></p>
<p>   - <strong>Type:</strong> Boolean</p>
<p>   - <strong>Description:</strong> Indicates if the candidate is considered a language.</p>
<p>4. <strong>HasSyntax</strong></p>
<p>   - <strong>Type:</strong> Boolean</p>
<p>   - <strong>Description:</strong> Indicates if the language candidate has syntax and/or grammar.</p>
<p>5. <strong>CanBeHeld</strong></p>
<p>   - <strong>Type:</strong> Boolean</p>
<p>   - <strong>Description:</strong> Indicates if the candidate is physical/material and could theoretically be held.</p>
<p>6. <strong>DistanceFromConcept</strong></p>
<p>   - <strong>Type:</strong> Integer</p>
<p>   - <strong>Description:</strong> A numerical representation of how closely the candidate relates to a specific concept.</p>
<p>7. <strong>HasIdentity</strong></p>
<p>   - <strong>Type:</strong> Boolean</p>
<p>   - <strong>Description:</strong> Indicates if the candidate can be assigned a unique identifier.</p>
<p>8. <strong>IsParsed</strong></p>
<p>   - <strong>Type:</strong> Boolean</p>
<p>   - <strong>Description:</strong> Indicates if the knowledge/information requires parsing before meaning can be extracted.</p>
<p>9. <strong>ResolvesToAnAST</strong></p>
<p>   - <strong>Type:</strong> Boolean</p>
<p>   - <strong>Description:</strong> Indicates if the knowledge/information can be represented as an Abstract Syntax Tree (AST).</p>
<p>10. <strong>HasLinearDecodingPressure</strong></p>
<p>    - <strong>Type:</strong> Boolean</p>
<p>    - <strong>Description:</strong> Indicates if the candidate has linear decoding pressure.</p>
<p>11. <strong>IsStableOntologyReference</strong></p>
<p>    - <strong>Type:</strong> Boolean</p>
<p>    - <strong>Description:</strong> Indicates if the candidate is a stable ontology reference.</p>
<p>12. <strong>IsOpenWorld</strong></p>
<p>    - <strong>Type:</strong> Boolean</p>
<p>    - <strong>Description:</strong> Indicates if the candidate is considered to be in an open world context.</p>
<p>13. <strong>IsClosedWorld</strong></p>
<p>    - <strong>Type:</strong> Boolean</p>
<p>    - <strong>Description:</strong> Indicates if the candidate is considered to be in a closed world context.</p>
<p>14. <strong>IsDescriptionOf</strong></p>
<p>    - <strong>Type:</strong> Boolean</p>
<p>    - <strong>Description:</strong> Indicates if the candidate describes a concept rather than being the concept itself.</p>
<h4>Calculated Fields (Type: Calculated)</h4>
<p>1. <strong>HasGrammar</strong></p>
<p>   - <strong>Description:</strong> Determines if the candidate has grammar based on the presence of syntax.</p>
<p>   - <strong>Calculation:</strong> If `HasSyntax` is true, then `HasGrammar` is true.</p>
<p>   - <strong>Formula:</strong> `={{HasSyntax}} = TRUE()`</p>
<p>   - <strong>Example:</strong> If `HasSyntax` for "English" is true, then `HasGrammar` will also be true.</p>
<p>2. <strong>Question</strong></p>
<p>   - <strong>Description:</strong> Constructs a question that could be posed to a group of people, family feud style.</p>
<p>   - <strong>Calculation:</strong> The question is formed by concatenating "Is ", the candidate's name, and " a language?".</p>
<p>   - <strong>Formula:</strong> `="Is " & {{Name}} & " a language?"`</p>
<p>   - <strong>Example:</strong> For "English", the question will be "Is English a language?".</p>
<p>3. <strong>PredictedAnswer</strong></p>
<p>   - <strong>Description:</strong> Predicts the most popular answer based on various criteria.</p>
<p>   - <strong>Calculation:</strong> The answer is true if all of the following conditions are met:</p>
<p>     - Has syntax</p>
<p>     - Is parsed</p>
<p>     - Is a description of a concept</p>
<p>     - Has linear decoding pressure</p>
<p>     - Resolves to an AST</p>
<p>     - Is a stable ontology reference</p>
<p>     - Cannot be held</p>
<p>     - Has no identity</p>
<p>   - <strong>Formula:</strong> `=AND({{HasSyntax}}, {{IsParsed}}, {{IsDescriptionOf}}, {{HasLinearDecodingPressure}}, {{ResolvesToAnAST}}, {{IsStableOntologyReference}}, NOT({{CanBeHeld}}), NOT({{HasIdentity}}))`</p>
<p>   - <strong>Example:</strong> For "English", if all conditions are met, `PredictedAnswer` will be true.</p>
<p>4. <strong>PredictionPredicates</strong></p>
<p>   - <strong>Description:</strong> Summarizes the predicates that led to the prediction.</p>
<p>   - <strong>Calculation:</strong> Concatenates the results of various predicates into a descriptive string.</p>
<p>   - <strong>Formula:</strong> </p>
<p>     ```</p>
<p>     =IF({{HasSyntax}}, "Has Syntax", "No Syntax") & " & " & </p>
<p>     IF({{IsParsed}}, "Requires Parsing", "No Parsing Needed") & " & " & </p>
<p>     IF({{IsDescriptionOf}}, "Describes the thing", "Is the Thing") & " & " & </p>
<p>     IF({{HasLinearDecodingPressure}}, "Has Linear Decoding Pressure", "No Decoding Pressure") & " & " & </p>
<p>     IF({{ResolvesToAnAST}}, "Resolves to AST", "No AST") & ", " & </p>
<p>     IF({{IsStableOntologyReference}}, "Is Stable Ontology", "Not 'Ontology'") & " AND " & </p>
<p>     IF({{CanBeHeld}}, "Can Be Held", "Can't Be Held") & ", " & </p>
<p>     IF({{HasIdentity}}, "Has Identity", "Has no Identity")</p>
<p>     ```</p>
<p>   - <strong>Example:</strong> For "English", the predicates will yield a string summarizing its characteristics.</p>
<p>5. <strong>PredictionFail</strong></p>
<p>   - <strong>Description:</strong> Provides a message explaining any mismatch between the predicted answer and the actual classification.</p>
<p>   - <strong>Calculation:</strong> If the predicted answer does not match `IsLanguage`, it constructs an explanation message.</p>
<p>   - <strong>Formula:</strong> </p>
<p>     ```</p>
<p>     =IF(NOT({{PredictedAnswer}} = {{IsLanguage}}),</p>
<p>       {{Name}} & " " & IF({{PredictedAnswer}}, "Is", "Isn't") & " a Family Feud Language, but " & </p>
<p>       IF({{IsLanguage}}, "Is", "Is Not") & " marked as a 'Language Candidate.'", "") & </p>
<p>       IF({{IsOpenClosedWorldConflicted}}, " - Open World vs. Closed World Conflict.", "")</p>
<p>     ```</p>
<p>   - <strong>Example:</strong> If "A Coffee Mug" is predicted to be a language but is marked as not being one, the output will indicate the mismatch.</p>
<p>6. <strong>IsDescriptionOf</strong></p>
<p>   - <strong>Description:</strong> Determines if the candidate describes a concept based on its distance from the concept.</p>
<p>   - <strong>Calculation:</strong> If `DistanceFromConcept` is greater than 1, then it is a description.</p>
<p>   - <strong>Formula:</strong> `={{DistanceFromConcept}} > 1`</p>
<p>   - <strong>Example:</strong> If "English" has a `DistanceFromConcept` of 2, then `IsDescriptionOf` will be true.</p>
<p>7. <strong>IsOpenClosedWorldConflicted</strong></p>
<p>   - <strong>Description:</strong> Checks for conflicts between open and closed world designations.</p>
<p>   - <strong>Calculation:</strong> True if both `IsOpenWorld` and `IsClosedWorld` are true.</p>
<p>   - <strong>Formula:</strong> `=AND({{IsOpenWorld}}, {{IsClosedWorld}})`</p>
<p>   - <strong>Example:</strong> If both flags are true for "Falsifier A", then this field will indicate a conflict.</p>
<p>8. <strong>RelationshipToConcept</strong></p>
<p>   - <strong>Description:</strong> Defines the relationship of the candidate to a concept based on its distance.</p>
<p>   - <strong>Calculation:</strong> If `DistanceFromConcept` equals 1, it is a mirror; otherwise, it describes the concept.</p>
<p>   - <strong>Formula:</strong> `=IF({{DistanceFromConcept}} = 1, "IsMirrorOf", "IsDescriptionOf")`</p>
<p>   - <strong>Example:</strong> For "A UML File" with a `DistanceFromConcept` of 2, the output will be "IsDescriptionOf".</p>
<h3>Conclusion</h3>
<p>This specification outlines how to compute the calculated fields for the `LanguageCandidates` entity within the rulebook. By following the provided formulas and examples, users can derive the necessary values for each candidate without needing to reference the original formulas directly.</p></div>
            </div>
        </div>
    </main>
    <script>
const themeToggle = document.getElementById('theme-toggle');
const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
function getInitialTheme() { try { if (window.parent && window.parent.document.documentElement.dataset.theme) return window.parent.document.documentElement.dataset.theme; } catch (e) {} return prefersDark ? 'dark' : 'light'; }
document.documentElement.dataset.theme = getInitialTheme();
themeToggle.addEventListener('click', () => { document.documentElement.dataset.theme = document.documentElement.dataset.theme === 'dark' ? 'light' : 'dark'; });
window.addEventListener('message', (event) => { if (event.data.type === 'theme-change') document.documentElement.dataset.theme = event.data.theme; });
document.querySelectorAll('.tab').forEach(tab => { tab.addEventListener('click', () => { document.querySelectorAll('.tab').forEach(t => t.classList.remove('active')); document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active')); tab.classList.add('active'); document.getElementById(tab.dataset.tab).classList.add('active'); }); });
    </script>
</body>
</html>